{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuigiElo/DL-PINNs/blob/main/PINN_LaPlace2D_Pytorch_PDE_noPDE_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/dr-aheydari/SoftAdapt.git"
      ],
      "metadata": {
        "id": "yO0KzAbex8zS",
        "outputId": "8065a250-b0b2-4616-995c-64d796057352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/dr-aheydari/SoftAdapt.git\n",
            "  Cloning https://github.com/dr-aheydari/SoftAdapt.git to /tmp/pip-req-build-avahpljg\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dr-aheydari/SoftAdapt.git /tmp/pip-req-build-avahpljg\n",
            "  Resolved https://github.com/dr-aheydari/SoftAdapt.git to commit 5fc955f424052cc94824d7ba9d7e7ea564c8e254\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: findiff in /usr/local/lib/python3.10/dist-packages (from softadapt==0.0.5) (0.10.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from softadapt==0.0.5) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from softadapt==0.0.5) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->softadapt==0.0.5) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->softadapt==0.0.5) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->softadapt==0.0.5) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->softadapt==0.0.5) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->softadapt==0.0.5) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->softadapt==0.0.5) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->softadapt==0.0.5) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from findiff->softadapt==0.0.5) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from findiff->softadapt==0.0.5) (1.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.1->softadapt==0.0.5) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->softadapt==0.0.5) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from softadapt import SoftAdapt, NormalizedSoftAdapt, LossWeightedSoftAdapt"
      ],
      "metadata": {
        "id": "UOtCoIAKyJeP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Network"
      ],
      "metadata": {
        "id": "dFr0R0YdjZy7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mEp-TPTfBeuF",
        "outputId": "989c2f87-9dc2-497b-f544-3ac659ad5d80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel = LinearNN()\\ninput_tensor = torch.randn((10, 2))  # Example input with batch size 10 and 2 features\\noutput = model(input_tensor)\\n\\nprint(\"Input shape:\", input_tensor)\\nprint(\"Output shape:\", output)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SineActivation(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)\n",
        "\n",
        "class FNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_inputs: int = 2,\n",
        "        num_layers: int = 4,\n",
        "        num_neurons: int = 128,\n",
        "        dropout_rate: float = 0.5,  # Add dropout rate as a parameter\n",
        "        act: nn.Module = SineActivation(),\n",
        "    ) -> None:\n",
        "        \"\"\"Basic neural network architecture with linear layers\n",
        "\n",
        "        Args:\n",
        "            num_inputs (int, optional): the dimensionality of the input tensor\n",
        "            num_layers (int, optional): the number of hidden layers\n",
        "            num_neurons (int, optional): the number of neurons for each hidden layer\n",
        "            act (nn.Module, optional): the non-linear activation function to use for stitching\n",
        "                linear layers togeter\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_neurons = num_neurons\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # input layer\n",
        "        layers.append(nn.Linear(self.num_inputs, num_neurons))\n",
        "\n",
        "        # hidden layers with linear layer and activation\n",
        "        for i in range(num_layers):\n",
        "            layers.extend([nn.Linear(num_neurons, num_neurons),act])\n",
        "            # Add dropout after every other linear layer\n",
        "            #if i % 2 == 1:\n",
        "            #  layers.extend([nn.Linear(num_neurons, num_neurons),act])\n",
        "            #else:\n",
        "            #  layers.extend([nn.Linear(num_neurons, num_neurons), nn.Dropout(dropout_rate) ,act])\n",
        "\n",
        "\n",
        "        # output layer\n",
        "        layers.append(nn.Linear(num_neurons, 1))\n",
        "\n",
        "        # build the network\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x).squeeze()\n",
        "\n",
        "#network\n",
        "\n",
        "# Example usage:\n",
        "\"\"\"\n",
        "model = LinearNN()\n",
        "input_tensor = torch.randn((10, 2))  # Example input with batch size 10 and 2 features\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(\"Input shape:\", input_tensor)\n",
        "print(\"Output shape:\", output)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PINN Loss"
      ],
      "metadata": {
        "id": "0EgdELmFjcdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.autograd.functional as autograd\n",
        "\n",
        "def data_loss(predicted_data, target_data): #data has to be in z=0, 0<x<L\n",
        "        #target data is obtained using behaviour function\n",
        "        #return F.mse_loss(predicted_data, target_data)\n",
        "        criterion=torch.nn.L1Loss()\n",
        "        #criterion=torch.nn.MSELoss()\n",
        "        #target_data = target_data.view(-1, 1)  # Assuming you want a column vector\n",
        "        predicted_data =predicted_data.view(-1,1)\n",
        "        #print(\"TARGET: \",target_data.shape)\n",
        "        #print(\"PREDICTED :\",predicted_data.shape)\n",
        "        return criterion(predicted_data, target_data)\n",
        "\n",
        "def boundary_condition_loss(boundary_conditions,output):\n",
        "    # Customize based on your specific boundary conditions\n",
        "    # Example: Dirichlet boundary condition u(0, t) = g(t)\n",
        "    #loss_bc = F.mse_loss(boundary_conditions['bc1'], torch.zeros_like(output)) + F.mse_loss(boundary_conditions['bc2_0'], boundary_conditions['bc2_1'])\n",
        "    # instead of 0 it will be an array of zeros with the same length as the number of points we decide to use to calculate the loss\n",
        "    criterion=torch.nn.L1Loss()\n",
        "    #criterion=torch.nn.MSELoss()\n",
        "    loss_bc = criterion(boundary_conditions['bc1'], torch.zeros_like(output)) + criterion(boundary_conditions['bc2_0'], boundary_conditions['bc2_1'])\n",
        "    return loss_bc\n",
        "\n",
        "def periodic_boundary_loss(boundary_conditions):\n",
        "    # Customize based on your specific boundary conditions\n",
        "    # Example: Dirichlet boundary condition u(0, t) = g(t)\n",
        "    #loss_bc = F.mse_loss(boundary_conditions['bc1'], torch.zeros_like(output)) + F.mse_loss(boundary_conditions['bc2_0'], boundary_conditions['bc2_1'])\n",
        "    # instead of 0 it will be an array of zeros with the same length as the number of points we decide to use to calculate the loss\n",
        "    criterion=torch.nn.L1Loss()\n",
        "    #criterion=torch.nn.MSELoss()\n",
        "    loss_bc = criterion(boundary_conditions['bc2_0'], boundary_conditions['bc2_1'])\n",
        "    return loss_bc\n",
        "\n",
        "def bottom_velocity_boundary_loss(boundary_conditions,output):\n",
        "\n",
        "    criterion=torch.nn.L1Loss()\n",
        "    #criterion=torch.nn.MSELoss()\n",
        "    loss_bc = criterion(boundary_conditions['bc1'], torch.zeros_like(output))\n",
        "    return loss_bc\n",
        "\n",
        "def pde_loss(model,input_data): #here input_data has to be in -h(x)<x2<0\n",
        "#\n",
        "#    # Forward pass to get the function values\n",
        "#    output = model(input_data)\n",
        "#    pde_residuals=[]\n",
        "#    for o,d in zip(output,input_data):\n",
        "      # Compute the Hessian matrix\n",
        "#      hessian = compute_hessian(o, d)\n",
        "\n",
        "      # Extract elements corresponding to (0, 0) and (1, 1)\n",
        "      #hessian_00 = hessian[0, 0]\n",
        "      #hessian_11 = hessian[1, 1]\n",
        "\n",
        "      #pde_residuals.append(hessian_00+hessian_11)\n",
        "    pde_residuals_tensor = compute_hessian(model,input_data)\n",
        "    #print(f\"{hessians.sum(dim=1)}\")\n",
        "    #pde_residuals_tensor = hessians.sum(dim=1)\n",
        "    #pde_residuals_tensor = torch.cat([residual.unsqueeze(0) for residual in pde_residuals])\n",
        "\n",
        "    # Use torch.zeros_like with the tensor\n",
        "    #return F.mse_loss(pde_residuals_tensor, torch.zeros_like(pde_residuals_tensor))\n",
        "    criterion=torch.nn.L1Loss()\n",
        "    #criterion=torch.nn.MSELoss()\n",
        "    return criterion(pde_residuals_tensor, torch.zeros_like(pde_residuals_tensor))\n",
        "\n",
        "def compute_bc1(model,  x1,x2):\n",
        "\n",
        "    x1_tensor = torch.tensor(x1, dtype=torch.float32, requires_grad=True)\n",
        "    x2_tensor = torch.tensor(x2, dtype=torch.float32 , requires_grad=True)\n",
        "\n",
        "    # Concatenate x1 and x2 to form the input tensor\n",
        "    input_tensor = torch.stack([x1_tensor, x2_tensor], dim=1)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    output = model(input_tensor)\n",
        "\n",
        "    # Compute the gradient with respect to x2\n",
        "    grad_x2 = torch.autograd.grad(output, input_tensor,grad_outputs=torch.ones_like(output), create_graph=True)[0][:, 1]\n",
        "\n",
        "    # Initialize a list to store the gradients for each element of x2_tensor\n",
        "    #gradients_x2 = []\n",
        "\n",
        "    # Compute the gradient for each element of output with respect to x2_tensor\n",
        "    #for i in range(len(output)):\n",
        "    #    grad_x2_element = torch.autograd.grad(output[i], x2_tensor[i], create_graph=True)[0]\n",
        "    #    gradients_x2.append(grad_x2_element)\n",
        "\n",
        "    # Stack the gradients into a tensor\n",
        "    #grad_x2 = torch.stack(gradients_x2, dim=0)\n",
        "\n",
        "    #print(f'Gradient bottom: {grad_x2}')\n",
        "    return grad_x2\n",
        "\n",
        "def compute_bc2(model, input_tensor):\n",
        "\n",
        "    # Forward pass through the model\n",
        "    output = model(input_tensor)\n",
        "\n",
        "    #print(f'output: {output}')\n",
        "    return output\n",
        "\n",
        "#def compute_hessian(output, input_data):\n",
        "#    hessian = autograd.hessian(lambda x: output, input_data)\n",
        "#    return hessian\n",
        "\n",
        "def compute_hessian(model, input_data):\n",
        "    output = model(input_data)\n",
        "    # Calculate the first-order gradients\n",
        "    gradients = torch.autograd.grad(output, input_data, grad_outputs=torch.ones_like(output), create_graph=True)[0]\n",
        "\n",
        "    # Calculate the second-order gradients for each element of input_data\n",
        "    hessian = torch.zeros_like(input_data)\n",
        "    for i in range(input_data.shape[1]):\n",
        "      hessian[:, i] = torch.autograd.grad(gradients[:, i], input_data, grad_outputs=torch.ones_like(gradients[:, i]), create_graph=True)[0][:, i]\n",
        "\n",
        "    return hessian.sum(dim=1)\n",
        "#x1 and x2 have to represent the points we want to apply the constraints\n",
        "\n",
        "#boundary_conditions = {\n",
        "#    'bc1': compute_bc1(model, x1, -1),   # at x2=-1 the grad perpendicular to the bottom is null\n",
        "#    'bc2_0': compute_bc2(model, 0, x2),  # at x1 = 0\n",
        "#    'bc2_1': compute_bc2(model, 2, x2),  # at x1 = 2\n",
        "#}\n",
        "\n",
        "#total_loss = loss_data + loss_bc + loss_pde\n",
        "\n",
        "#the data we provide to calculate each loss is extremely important. It has to belong to the domain where the constraints/pde/function are applied."
      ],
      "metadata": {
        "id": "MqeERJMqgnsj"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions analytical calculation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sJ6e62UKfxMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# General parameters\n",
        "L = 2.0 # wave/x length\n",
        "h = 1 # depth\n",
        "T = 1 # period\n",
        "H = 0.05 # surface wave amplitude (I chose it arbitrarliy. Check p. 68 HRC)\n",
        "c = L/T # wave propagation velocity (p. 70 HCR)\n",
        "k = 2*np.pi/L # wave nr.\n",
        "w = 2*np.pi/T\n",
        "\n",
        "t = 0 # we are looking at a snapshot, so t is constant\n",
        "\n",
        "# Analytical solution (HRC p. 75)\n",
        "def behaviour_func(x):\n",
        "    return -H*c/2 * np.cosh(k*(x[:, 1:2]+h))/np.sinh(k*h) * np.sin(w*t - k*x[:, 0:1])\n",
        "\n",
        "def velocity_func(x):\n",
        "\n",
        "  u= H*c/2 * k * np.cosh(k*(x[:, 1:2]+h))/np.sinh(k*h) * np.cos(w*t-k*x[:, 0:1])\n",
        "  v= -H*c/2 * k * np.sinh(k*(x[:, 1:2]+h))/np.sinh(k*h) * np.sin(w*t - k*x[:, 0:1])\n",
        "\n",
        "  return u,v"
      ],
      "metadata": {
        "id": "UBRT3N3LfvZq"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate data"
      ],
      "metadata": {
        "id": "hRioq-8OfZgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def  generate_data_function_approximation(size): #data generator for data loss and can also be used to train the network\n",
        "  # Generate random values for x and t\n",
        "    x1 = np.random.uniform(0, 2, size)\n",
        "    x2 = np.zeros_like(x1)\n",
        "    #t = np.random.uniform(0, 1, size)\n",
        "\n",
        "    # Create input data by stacking x1, x2, and t\n",
        "    #input_data = np.column_stack((x1, x2, t))\n",
        "    input_data = np.column_stack((x1, x2))\n",
        "\n",
        "    # Compute the corresponding function values using the analytical solution\n",
        "    target_data = behaviour_func(input_data)\n",
        "\n",
        "    return input_data, target_data\n",
        "\n",
        "def generate_data_pde_loss(size):\n",
        "    # Generate random values for x1 and x2 within the specified PDE domain\n",
        "    x1 = np.random.uniform(0, L, size)\n",
        "    x2 = np.random.uniform(-h, 0, size)\n",
        "\n",
        "    # Create input data by stacking x1 and x2\n",
        "    input_data = np.column_stack((x1, x2))\n",
        "\n",
        "    return input_data\n",
        "\n",
        "def generate_periodic_boundary_data(size, x1_values):\n",
        "\n",
        "    x1_0=np.full(size,x1_values[0]);\n",
        "    x1_1=np.full(size,x1_values[1]);\n",
        "\n",
        "    # Generate random values for x2 within the specified range for each x1\n",
        "    x2 = np.random.uniform(-h, 0, size)\n",
        "\n",
        "    # Create input data by stacking x1 and x2\n",
        "    input_data_0=input_data = np.column_stack((x1_0, x2))\n",
        "    input_data_1 = np.column_stack((x1_1, x2))\n",
        "\n",
        "    return input_data_0,input_data_1\n",
        "def generate_data_bottom_velocity(size):\n",
        "\n",
        "    x2=np.full(size,-h);\n",
        "    x1 = np.random.uniform(0, 2, size)\n",
        "\n",
        "    # Create input data by stacking x1 and x2\n",
        "    input_data = np.column_stack((x1, x2))\n",
        "\n",
        "    return input_data\n",
        "\n",
        "#size = 10  # specify the desired size\n",
        "#input_data, target_data = generate_data_z0(size)\n",
        "#generate_pde_input_data(size)\n",
        "\n",
        "#x1_values = np.array([0, 2])  # specify the x1 values\n",
        "#input_data_0,input_data_1 = generate_periodic_boundary_data(size, x1_values)\n",
        "#print(input_data_0,input_data_1)\n",
        "#print(generate_bottom_velocity_data(10))"
      ],
      "metadata": {
        "id": "1n2MGP81fZER"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd.functional as autograd\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 10)\n",
        "        self.relu = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # Ensure the output is a scalar\n",
        "\n",
        "#def compute_hessian2(model, input_data):\n",
        "\n",
        "#    output=model(input_data)\n",
        "#    grad_x2 = torch.autograd.grad(output, input_data,grad_outputs=torch.ones_like(output), create_graph=True)[0][:, 1]\n",
        "#    hessian = torch.autograd.grad(grad_x2, input_data,grad_outputs=torch.ones_like(grad_x2), create_graph=True)[0][:, 1]\n",
        "#    return hessian\n",
        "\n",
        "def compute_hessian2(model, input_data):\n",
        "    output = model(input_data)\n",
        "    # Calculate the first-order gradients\n",
        "    gradients = torch.autograd.grad(output, input_data, grad_outputs=torch.ones_like(output), create_graph=True)[0]\n",
        "\n",
        "    # Calculate the second-order gradients for each element of input_data\n",
        "    hessian = torch.zeros_like(input_data)\n",
        "    for i in range(input_data.shape[1]):\n",
        "      hessian[:, i] = torch.autograd.grad(gradients[:, i], input_data, grad_outputs=torch.ones_like(gradients[:, i]), create_graph=True)[0][:, i]\n",
        "\n",
        "    return hessian.sum(dim=1)\n",
        "\n",
        "# Rest of your code remains the same\n",
        "\n",
        "# Create an instance of the neural network\n",
        "model = SimpleNN()\n",
        "\n",
        "# Generate some random input data\n",
        "input_data = torch.randn((10, 2), requires_grad=True)\n",
        "\n",
        "# Forward pass to get the function values\n",
        "#output = model(input_data)\n",
        "\n",
        "hessian = compute_hessian2(model, input_data)\n",
        "print(f'Hessian : {hessian}')\n",
        "# Compute the Hessian matrix\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVNUVfcZYTeq",
        "outputId": "0c361dd7-5203-4639-ae70-09a3a4f116d7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hessian : tensor([-0.0271, -0.0323,  0.0541, -0.0303,  0.2066, -0.0197,  0.1025,  0.1674,\n",
            "        -0.0454, -0.0192], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple neural network\n",
        "class Simple2DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple2DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc1(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = Simple2DNN()\n",
        "\n",
        "# Create a 2D input tensor\n",
        "x_input_2d = torch.tensor([[2.0, 3.0]], dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(x_input_2d)\n",
        "\n",
        "# Compute the gradient with respect to the first dimension (index 0)\n",
        "grad_input_1 = torch.autograd.grad(output, x_input_2d, grad_outputs=torch.ones_like(output), create_graph=True)[0][:, 0]\n",
        "\n",
        "# Print the results\n",
        "print(\"Input Tensor (2D):\")\n",
        "print(x_input_2d)\n",
        "print(\"\\nOutput Tensor:\")\n",
        "print(output)\n",
        "print(\"\\nGradient with respect to the First Dimension:\")\n",
        "print(grad_input_1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WqBCZplPP7G",
        "outputId": "4e05fd7e-e322-451c-d66a-ae039035a8e9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensor (2D):\n",
            "tensor([[2., 3.]], requires_grad=True)\n",
            "\n",
            "Output Tensor:\n",
            "tensor([[0.0950]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Gradient with respect to the First Dimension:\n",
            "tensor([-0.0819], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalize data"
      ],
      "metadata": {
        "id": "Ot71MaENYmWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(data_function_approximation,data_boundary_conditions_0,data_boundary_conditions_1,data_pde_loss,data_vel_bottom,data_size):\n",
        "  total_input_train_data=[]\n",
        "\n",
        "  total_input_train_data.extend(data_function_approximation)\n",
        "  total_input_train_data.extend(data_boundary_conditions_0)\n",
        "  total_input_train_data.extend(data_boundary_conditions_1)\n",
        "  total_input_train_data.extend(data_pde_loss)\n",
        "  total_input_train_data.extend(data_vel_bottom)\n",
        "\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  total_input_train_data_scaled= scaler.fit_transform(total_input_train_data)\n",
        "  #input_data_function_approximation = scaler.fit_transform(input_data_function_approximation)\n",
        "  #input_data_boundary_conditions_0 = scaler.fit_transform(input_data_boundary_conditions_0)\n",
        "  #input_data_boundary_conditions_1 = scaler.fit_transform(input_data_boundary_conditions_1)\n",
        "  #input_data_vel_bottom = scaler.fit_transform(input_data_vel_bottom)\n",
        "  #input_data_pde_loss = scaler.fit_transform(input_data_pde_loss)\n",
        "\n",
        "  #print(total_input_train_data_scaled[:,0])\n",
        "  #print(total_input_train_data_scaled[:,1])\n",
        "\n",
        "\n",
        "  input_data_function_approximation = total_input_train_data_scaled[0:data_size,:]\n",
        "  input_data_boundary_conditions_0 = total_input_train_data_scaled[data_size:2*data_size,:]\n",
        "  input_data_boundary_conditions_1 = total_input_train_data_scaled[2*data_size:3*data_size,:]\n",
        "  input_data_pde_loss = total_input_train_data_scaled[3*data_size:4*data_size,:]\n",
        "  input_data_vel_bottom = total_input_train_data_scaled[4*data_size:5*data_size,:]\n",
        "\n",
        "  return input_data_function_approximation, input_data_boundary_conditions_0,input_data_boundary_conditions_1, input_data_pde_loss, input_data_vel_bottom"
      ],
      "metadata": {
        "id": "YK5HO6n0Ylmi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Network"
      ],
      "metadata": {
        "id": "IZrKJfhTrh5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters and data"
      ],
      "metadata": {
        "id": "tTHaNcaAu8kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Define parameters\n",
        "num_epochs = 5000\n",
        "learning_rate = 10**-4\n",
        "momentum = 0.9\n",
        "#alpha = 1.0  # Weight for data loss\n",
        "#beta = 1.0   # Weight for periodic boundary condition loss\n",
        "#epsilon = 1.0 # Weight for bottom velocity boundary condition loss\n",
        "#gamma = 1.0  # Weight for PDE loss\n",
        "\n",
        "\n",
        "# Create the model and optimizer\n",
        "model = FNN()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "training_data_size=20\n",
        "test_data_size=5\n",
        "\n",
        "# Generate training data for each component of the PINN loss\n",
        "input_data_function_approximation, target_data_function_approximation = generate_data_function_approximation(training_data_size)\n",
        "input_data_boundary_conditions_0, input_data_boundary_conditions_1 = generate_periodic_boundary_data(training_data_size, x1_values=[0, 2])\n",
        "input_data_pde_loss = generate_data_pde_loss(training_data_size)\n",
        "input_data_vel_bottom = generate_data_bottom_velocity(training_data_size)\n",
        "\n",
        "\n",
        "\n",
        "#normalize data\n",
        "input_data_function_approximation, input_data_boundary_conditions_0,input_data_boundary_conditions_1, input_data_pde_loss, input_data_vel_bottom = normalize_data(input_data_function_approximation, input_data_boundary_conditions_0,input_data_boundary_conditions_1, input_data_pde_loss, input_data_vel_bottom,training_data_size)\n",
        "\n",
        "# Change 1: Create a SoftAdapt object (with your desired variant)\n",
        "softadapt_object = LossWeightedSoftAdapt(beta=0.1)\n",
        "\n",
        "# Change 2: Define how often SoftAdapt calculate weights for the loss components\n",
        "epochs_to_make_updates = 5\n",
        "\n",
        "# Change 3: Initialize lists to keep track of loss values over the epochs we defined above\n",
        "values_of_component_1 = []\n",
        "values_of_component_2 = []\n",
        "values_of_component_3 = []\n",
        "values_of_component_4 = []\n",
        "# Initializing adaptive weights to all ones.\n",
        "adapt_weights = torch.tensor([1,1,1,1])"
      ],
      "metadata": {
        "id": "SSgSLa7HwCO7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "-PoBrdvovG6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store training losses for plotting\n",
        "loss_pde_list = []\n",
        "#loss_bc_list = []\n",
        "loss_data_list = []\n",
        "total_loss_list = []\n",
        "loss_per_bc_list=[]\n",
        "loss_bot_vel_bc_list=[]\n",
        "\n",
        "# Lists to store test losses for plotting\n",
        "test_loss_pde_list = []\n",
        "#test_loss_bc_list = []\n",
        "test_loss_data_list = []\n",
        "test_total_loss_list = []\n",
        "test_loss_per_bc_list=[]\n",
        "test_loss_bot_vel_bc_list=[]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "    # Forward pass for each component\n",
        "    predicted_data_function_approximation = model(torch.from_numpy(input_data_function_approximation).float())\n",
        "\n",
        "    # Compute losses\n",
        "    loss_function_approximation = data_loss(predicted_data_function_approximation, torch.Tensor(target_data_function_approximation).float())\n",
        "\n",
        "    boundary_conditions = {\n",
        "      'bc1': compute_bc1(model, input_data_vel_bottom[:,0], input_data_vel_bottom[:,1]),   # at x2=-1 the grad perpendicular to the bottom is null\n",
        "      'bc2_0': compute_bc2(model,torch.Tensor( input_data_boundary_conditions_0)),  # at x1 = 0\n",
        "      'bc2_1': compute_bc2(model,torch.Tensor( input_data_boundary_conditions_1)),  # at x1 = 2\n",
        "    }\n",
        "\n",
        "    #loss_boundary_conditions = beta* boundary_condition_loss(boundary_conditions,predicted_data_function_approximation)\n",
        "\n",
        "    loss_periodic_boundary = periodic_boundary_loss(boundary_conditions)\n",
        "\n",
        "    loss_pde_loss= pde_loss(model,torch.tensor(input_data_pde_loss, requires_grad = True, dtype=torch.float))\n",
        "\n",
        "    loss_vel_bottom_boundary = bottom_velocity_boundary_loss(boundary_conditions,predicted_data_function_approximation)\n",
        "\n",
        "    # Keeping track of each loss component\n",
        "    values_of_component_1.append(loss_function_approximation)\n",
        "    values_of_component_2.append( loss_periodic_boundary )\n",
        "    values_of_component_3.append(loss_pde_loss)\n",
        "    values_of_component_4.append(loss_vel_bottom_boundary)\n",
        "\n",
        "    # Change 4: Make sure `epochs_to_make_change` have passed before calling SoftAdapt.\n",
        "    if epoch % epochs_to_make_updates == 0 and epoch != 0:\n",
        "        adapt_weights = softadapt_object.get_component_weights(torch.tensor(values_of_component_1),\n",
        "                                                                 torch.tensor(values_of_component_2),\n",
        "                                                                 torch.tensor(values_of_component_3),\n",
        "                                                                torch.tensor(values_of_component_4),\n",
        "                                                                 verbose=False,\n",
        "                                                                   )\n",
        "        # Resetting the lists to start fresh (this part is optional)\n",
        "        values_of_component_1 = []\n",
        "        values_of_component_2 = []\n",
        "        values_of_component_3 = []\n",
        "        values_of_component_4 = []\n",
        "\n",
        "\n",
        "\n",
        "    # Total loss\n",
        "    #total_loss = loss_function_approximation + loss_boundary_conditions + loss_pde_loss\n",
        "\n",
        "    #total_loss = loss_function_approximation + loss_periodic_boundary + loss_vel_bottom_boundary + loss_pde_loss\n",
        "\n",
        "    total_loss = adapt_weights[0] * loss_function_approximation + adapt_weights[1]*loss_periodic_boundary + adapt_weights[2]*loss_pde_loss + adapt_weights[3]*loss_vel_bottom_boundary\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    # Append losses to the lists\n",
        "    loss_pde_list.append(loss_pde_loss.item())\n",
        "    #loss_bc_list.append(loss_boundary_conditions.item())\n",
        "    loss_per_bc_list.append(loss_periodic_boundary.item())\n",
        "    loss_bot_vel_bc_list.append(loss_vel_bottom_boundary.item())\n",
        "    loss_data_list.append(loss_function_approximation.item())\n",
        "    total_loss_list.append(total_loss.item())\n",
        "\n",
        "    # Generate test data for each component of the PINN loss\n",
        "    test_input_data_function_approximation, test_target_data_function_approximation = generate_data_function_approximation(test_data_size)\n",
        "    test_input_data_boundary_conditions_0, test_input_data_boundary_conditions_1 = generate_periodic_boundary_data(test_data_size, x1_values=[0, 2])\n",
        "    test_input_data_pde_loss = generate_data_pde_loss(test_data_size)\n",
        "    test_input_data_vel_bottom = generate_data_bottom_velocity(test_data_size)\n",
        "\n",
        "    #normalize data\n",
        "    test_input_data_function_approximation, test_input_data_boundary_conditions_0, test_input_data_boundary_conditions_1, test_input_data_pde_loss, test_input_data_vel_bottom = normalize_data(test_input_data_function_approximation, test_input_data_boundary_conditions_0, test_input_data_boundary_conditions_1, test_input_data_pde_loss, test_input_data_vel_bottom,test_data_size)\n",
        "    # Test loop\n",
        "    # Forward pass for each component\n",
        "    test_predicted_data_function_approximation = model(torch.Tensor(test_input_data_function_approximation))\n",
        "\n",
        "    # Compute test losses\n",
        "    test_loss_function_approximation = data_loss(test_predicted_data_function_approximation, torch.Tensor(test_target_data_function_approximation))\n",
        "\n",
        "    test_boundary_conditions = {\n",
        "        'bc1': compute_bc1(model, test_input_data_vel_bottom[:, 0], test_input_data_vel_bottom[:, 1]),\n",
        "        'bc2_0': compute_bc2(model, torch.Tensor(test_input_data_boundary_conditions_0)),\n",
        "        'bc2_1': compute_bc2(model, torch.Tensor(test_input_data_boundary_conditions_1)),\n",
        "    }\n",
        "\n",
        "    #test_loss_boundary_conditions = beta*boundary_condition_loss(test_boundary_conditions, test_predicted_data_function_approximation)\n",
        "\n",
        "    test_loss_periodic_boundary = periodic_boundary_loss(test_boundary_conditions)\n",
        "\n",
        "    test_loss_vel_bottom_boundary = bottom_velocity_boundary_loss(test_boundary_conditions,test_predicted_data_function_approximation)\n",
        "\n",
        "\n",
        "    test_loss_pde_loss = pde_loss(model, torch.tensor(test_input_data_pde_loss, requires_grad = True, dtype=torch.float))\n",
        "\n",
        "    # Total test loss\n",
        "    #test_total_loss = test_loss_function_approximation + test_loss_boundary_conditions + test_loss_pde_loss\n",
        "    test_total_loss = test_loss_function_approximation + test_loss_periodic_boundary + test_loss_vel_bottom_boundary + test_loss_pde_loss\n",
        "\n",
        "    # Append test losses to the lists\n",
        "    test_loss_pde_list.append(test_loss_pde_loss.item())\n",
        "    #test_loss_bc_list.append(test_loss_boundary_conditions.item())\n",
        "    test_loss_bot_vel_bc_list.append(test_loss_vel_bottom_boundary.item())\n",
        "    test_loss_per_bc_list.append(test_loss_periodic_boundary.item())\n",
        "    test_loss_data_list.append(test_loss_function_approximation.item())\n",
        "    test_total_loss_list.append(test_total_loss.item())\n",
        "\n",
        "    # Print loss for monitoring\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}/{num_epochs},train PDE Loss: {loss_pde_loss}')\n",
        "        #print(f'Epoch {epoch}/{num_epochs},train Boundaries Loss: {loss_boundary_conditions}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},train Periodic Boundary Loss: {loss_periodic_boundary}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},train Bottom Velocity Boundary Loss: {loss_vel_bottom_boundary}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},train Data Loss: {loss_function_approximation}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},train Total Loss: {total_loss.item()}')\n",
        "        print(\"------------------------------------------------------------\")\n",
        "        print(f'Epoch {epoch}/{num_epochs},test PDE Loss: {test_loss_pde_loss}')\n",
        "        #print(f'Epoch {epoch}/{num_epochs},test Boundaries Loss: {test_loss_boundary_conditions}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},test Periodic Boundary Loss: {test_loss_periodic_boundary}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},test Bottom Velocity Boundary Loss: {test_loss_vel_bottom_boundary}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},test Data Loss: {test_loss_function_approximation}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},test Total Loss: {test_total_loss.item()}')\n",
        "        print(\"#############################################################\")\n",
        "        print(\"#############################################################\")\n",
        "# After training, you can use the trained model for predictions or further analysis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq5DxicKWaXP",
        "outputId": "952c7524-915a-4dd8-abf3-f09de153a454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5000,train PDE Loss: 0.0014444654807448387\n",
            "Epoch 0/5000,train Periodic Boundary Loss: 0.061620503664016724\n",
            "Epoch 0/5000,train Bottom Velocity Boundary Loss: 0.002750216983258724\n",
            "Epoch 0/5000,train Data Loss: 0.04929381608963013\n",
            "Epoch 0/5000,train Total Loss: 0.11510900408029556\n",
            "------------------------------------------------------------\n",
            "Epoch 0/5000,test PDE Loss: 0.0013421621406450868\n",
            "Epoch 0/5000,test Periodic Boundary Loss: 0.024099979549646378\n",
            "Epoch 0/5000,test Bottom Velocity Boundary Loss: 0.0006688704597763717\n",
            "Epoch 0/5000,test Data Loss: 0.04274984821677208\n",
            "Epoch 0/5000,test Total Loss: 0.06886085867881775\n",
            "#############################################################\n",
            "#############################################################\n",
            "Epoch 100/5000,train PDE Loss: 0.002175831701606512\n",
            "Epoch 100/5000,train Periodic Boundary Loss: 0.015153465792536736\n",
            "Epoch 100/5000,train Bottom Velocity Boundary Loss: 0.002453214954584837\n",
            "Epoch 100/5000,train Data Loss: 0.02469458058476448\n",
            "Epoch 100/5000,train Total Loss: 0.0197248194964084\n",
            "------------------------------------------------------------\n",
            "Epoch 100/5000,test PDE Loss: 0.003301431890577078\n",
            "Epoch 100/5000,test Periodic Boundary Loss: 0.020626390352845192\n",
            "Epoch 100/5000,test Bottom Velocity Boundary Loss: 0.0012133170384913683\n",
            "Epoch 100/5000,test Data Loss: 0.02609546110033989\n",
            "Epoch 100/5000,test Total Loss: 0.0512365996837616\n",
            "#############################################################\n",
            "#############################################################\n",
            "Epoch 200/5000,train PDE Loss: 0.0024900988209992647\n",
            "Epoch 200/5000,train Periodic Boundary Loss: 0.010085257701575756\n",
            "Epoch 200/5000,train Bottom Velocity Boundary Loss: 0.006499274168163538\n",
            "Epoch 200/5000,train Data Loss: 0.020477643236517906\n",
            "Epoch 200/5000,train Total Loss: 0.014454033594109976\n",
            "------------------------------------------------------------\n",
            "Epoch 200/5000,test PDE Loss: 0.0029550245963037014\n",
            "Epoch 200/5000,test Periodic Boundary Loss: 0.0089200334623456\n",
            "Epoch 200/5000,test Bottom Velocity Boundary Loss: 0.005784052424132824\n",
            "Epoch 200/5000,test Data Loss: 0.020512061193585396\n",
            "Epoch 200/5000,test Total Loss: 0.03817117214202881\n",
            "#############################################################\n",
            "#############################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Losses plots"
      ],
      "metadata": {
        "id": "CBpD1TJy91QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_range = range(0, num_epochs, 1)\n",
        "# Plotting PDE Loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs_range, test_loss_pde_list, label='Test PDE Loss', alpha=.5)\n",
        "plt.plot(epochs_range, loss_pde_list, label='PDE Loss')\n",
        "plt.title('PDE Loss Over Training Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting Periodic Boundary Loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs_range, test_loss_per_bc_list, label='Test Periodic Boundary Loss', alpha=.5)\n",
        "plt.plot(epochs_range, loss_per_bc_list, label='Periodic Boundary Loss')\n",
        "plt.title('Boundaries Loss Over Training Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting Boundaries Loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs_range, test_loss_bot_vel_bc_list, label='Test Boundaries Loss', alpha=.5)\n",
        "plt.plot(epochs_range, loss_bot_vel_bc_list, label='Boundaries Loss')\n",
        "plt.title('Boundaries Loss Over Training Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting Data Loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs_range, test_loss_data_list, label='Test Data Loss', alpha=.5)\n",
        "plt.plot(epochs_range, loss_data_list, label='Data Loss')\n",
        "plt.title('Data Loss Over Training Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting Total Loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs_range, test_total_loss_list, label='Test Total Loss', alpha=.5)\n",
        "plt.plot(epochs_range, total_loss_list, label='Total Loss')\n",
        "plt.title('Total Loss Over Training Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ocYgkxdkz_s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3D Plot with test data"
      ],
      "metadata": {
        "id": "o_01zglp-Awd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_test_data = []\n",
        "total_test_data_scaled = []\n",
        "test_data_size=50\n",
        "#generate test data\n",
        "test_input_data_function_approximation, test_target_data_function_approximation = generate_data_function_approximation(test_data_size)\n",
        "test_input_data_boundary_conditions_0, test_input_data_boundary_conditions_1 = generate_periodic_boundary_data(test_data_size, x1_values=[0, 2])\n",
        "test_input_data_pde_loss = generate_data_pde_loss(test_data_size)\n",
        "test_input_data_vel_bottom = generate_data_bottom_velocity(test_data_size)\n",
        "\n",
        "#WARNING: the values added here must be the same as in standardized data\n",
        "#total_test_data.extend(test_input_data_function_approximation)\n",
        "total_test_data.extend(test_input_data_boundary_conditions_0)\n",
        "total_test_data.extend(test_input_data_boundary_conditions_1)\n",
        "#total_test_data.extend(test_input_data_pde_loss)\n",
        "#total_test_data.extend(test_input_data_vel_bottom)\n",
        "\n",
        "#normalize data\n",
        "test_input_data_function_approximation, test_input_data_boundary_conditions_0, test_input_data_boundary_conditions_1, test_input_data_pde_loss, test_input_data_vel_bottom = normalize_data(test_input_data_function_approximation, test_input_data_boundary_conditions_0, test_input_data_boundary_conditions_1, test_input_data_pde_loss, test_input_data_vel_bottom,test_data_size)\n",
        "\n",
        "\n",
        "#add standardized data\n",
        "#WARINING: the values added here must be the same as above\n",
        "#total_test_data_scaled.extend(test_input_data_function_approximation)\n",
        "total_test_data_scaled.extend(test_input_data_boundary_conditions_0)\n",
        "total_test_data_scaled.extend(test_input_data_boundary_conditions_1)\n",
        "#total_test_data_scaled.extend(test_input_data_pde_loss)\n",
        "#total_test_data_scaled.extend(test_input_data_vel_bottom)\n",
        "\n",
        "total_test_data = torch.Tensor(total_test_data)\n",
        "total_test_data_scaled = torch.Tensor(total_test_data_scaled)\n",
        "# Convert the list to a PyTorch tensor\n",
        "output2= model(total_test_data_scaled)\n",
        "\n",
        "print(output2.shape,(total_test_data_scaled.shape))\n",
        "print(total_test_data.shape)"
      ],
      "metadata": {
        "id": "NbWjxTSV9-VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = total_test_data[:,0]\n",
        "y = total_test_data[:,1]\n",
        "print(x,y)"
      ],
      "metadata": {
        "id": "B6k58k16CR80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming total_test_data[0] and total_test_data[1] are 1D arrays\n",
        "x = total_test_data[:,0]\n",
        "y = total_test_data[:,1]\n",
        "z = output2.detach().numpy()  # Convert the torch Tensor to a NumPy array\n",
        "\n",
        "\n",
        "# Create a 3D plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Scatter plot\n",
        "ax.scatter(x,z, y, c='r', marker='o')\n",
        "\n",
        "# Set labels\n",
        "ax.set_xlabel('Length')\n",
        "ax.set_ylabel('Output')\n",
        "ax.set_zlabel('Deepness')\n",
        "\n",
        "# Set title\n",
        "ax.set_title('3D Scatter Plot of Output')\n",
        "\n",
        "#ax.view_init(elev=90, azim=90)  # better angle to analyze z=0\n",
        "ax.view_init(elev=0, azim=0)   #better angle to analyze periodic boundaries\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6j2wmv8S_w8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analytical (z=0) vs Predicted (z=0)"
      ],
      "metadata": {
        "id": "FTTkhdJZKlLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming total_test_data[0] and total_test_data[1] are 1D arrays\n",
        "x = test_input_data_function_approximation[:,0]\n",
        "y = test_input_data_function_approximation[:,1]\n",
        "zt = test_target_data_function_approximation  # Convert the torch Tensor to a NumPy array\n",
        "zp = model(torch.Tensor(test_input_data_function_approximation))\n",
        "zp= zp.detach().numpy()\n",
        "\n",
        "# Create a 3D plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Scatter plot\n",
        "ax.scatter(x,zt, y, c='r', marker='o', label='Target data')\n",
        "ax.scatter(x,zp, y, c='b', marker='o', label='Predicted data')\n",
        "\n",
        "ax.legend()\n",
        "# Set labels\n",
        "ax.set_xlabel('Length')\n",
        "ax.set_ylabel('Output')\n",
        "ax.set_zlabel('Deepness')\n",
        "\n",
        "# Set title\n",
        "ax.set_title('3D Scatter Plot of Output')\n",
        "\n",
        "ax.view_init(elev=90, azim=90)  # You can adjust the elev and azim values\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mv027OXSyeBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_output_grad(input):\n",
        "\n",
        "    # Forward pass through the model\n",
        "    output = model(input.requires_grad_())\n",
        "\n",
        "    # Compute gradients with respect to both x1_tensor and x2_tensor using a single call\n",
        "    grads = torch.autograd.grad(output, input, grad_outputs=torch.ones_like(output), create_graph=True)[0]\n",
        "\n",
        "    grad_x1 = grads[:, 0]\n",
        "    grad_x2 = grads[:, 1]\n",
        "\n",
        "    return grad_x1, grad_x2"
      ],
      "metadata": {
        "id": "h68dShJm5NoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Todo: Implement with 25 meshgrid better picture but does not work with the compute_output function\n",
        "x = np.linspace(0,2,10)\n",
        "y = np.linspace(0,-1,10)\n",
        "X,Y  = np.meshgrid(x,y)\n",
        "\n",
        "positions = np.vstack([X.ravel(), Y.ravel()])\n",
        "print(positions.shape)\n",
        "\n",
        "# Stack X and Y vertically\n",
        "data = np.vstack([X.flatten(), Y.flatten()]).T\n",
        "\n",
        "# Create and fit the scaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Calculate velocity of water for specific x,y direction\n",
        "u_analytic, v_analytic  = velocity_func(positions.T)\n",
        "u_network, v_network = compute_output_grad(torch.Tensor(scaled_data))\n",
        "u_network = u_network.detach().numpy()\n",
        "v_network = v_network.detach().numpy()\n",
        "\n",
        "\n",
        "fig , axs = plt.subplots(1,2, figsize = (15,5))\n",
        "axs[0].quiver(X,Y,u_network, v_network, v_network**2+u_network**2 )\n",
        "axs[1].quiver(X,Y,u_analytic,v_analytic,(v_analytic**2+u_analytic**2))\n",
        "axs[1].set_title('Analytic function results')\n",
        "axs[0].set_title('Network function results')\n",
        "\n",
        "#total_test_data = torch.Tensor(total_test_data)\n",
        "#total_test_data_scaled = torch.Tensor(total_test_data_scaled)\n",
        "# Convert the list to a PyTorch tensor"
      ],
      "metadata": {
        "id": "QxRX-7avW4lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotVelocity(total_test_data,total_test_data_scaled,prediction, ax):\n",
        "\n",
        "  if prediction:\n",
        "    u_values, v_values = compute_output_grad(total_test_data_scaled)\n",
        "  else:\n",
        "    u_values, v_values = velocity_func(total_test_data.detach().numpy())\n",
        "\n",
        "  # Plotting\n",
        "  x_values = total_test_data[:, 0].detach().numpy()\n",
        "  y_values = total_test_data[:, 1].detach().numpy()\n",
        "  if prediction:\n",
        "    u_values = u_values.detach().numpy()\n",
        "    v_values = v_values.detach().numpy()\n",
        "\n",
        "\n",
        "\n",
        "  # Plotting the vector field\n",
        "  ax.quiver(x_values, y_values, u_values, v_values, scale=.5, angles = 'xy',color='blue', width=0.002)\n",
        "  if prediction:\n",
        "    ax.set_title('Predicted Vector Field Plot')\n",
        "  else:\n",
        "    ax.set_title('Analytical Vector Field Plot')\n",
        "\n",
        "\n",
        "  return torch.from_numpy(u_values).view(-1,1), torch.from_numpy(v_values).view(-1,1)\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,6 ))\n",
        "u_predicted,v_predicted=plotVelocity(total_test_data,total_test_data_scaled,True,ax = axs[0])\n",
        "u_analytical,v_analytical= plotVelocity(total_test_data,total_test_data_scaled,False,ax = axs[1])\n",
        "\n",
        "#print(u_analytical)\n",
        "#print(u_predicted)\n",
        "#print(total_test_data)"
      ],
      "metadata": {
        "id": "Qt07xNl45HUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=torch.nn.L1Loss()\n",
        "\n",
        "u_loss=criterion(torch.Tensor(u_analytical),u_predicted)\n",
        "\n",
        "v_loss=criterion(torch.Tensor(v_analytical), v_predicted)\n",
        "\n",
        "print(f'Relative error u velocity: {abs(u_loss)/abs(torch.mean(torch.Tensor(u_analytical)))*100}%')\n",
        "print(f'Relative error v velocity: {abs(v_loss)/abs(torch.mean(torch.Tensor(v_analytical)))*100}%')"
      ],
      "metadata": {
        "id": "R3s-x8V_o-4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#No PDE"
      ],
      "metadata": {
        "id": "xUjlVKTfXi5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data_no_PDE(data_function_approximation,data_boundary_conditions_0,data_boundary_conditions_1,data_vel_bottom,data_size):\n",
        "  total_input_train_data=[]\n",
        "\n",
        "  total_input_train_data.extend(data_function_approximation)\n",
        "  total_input_train_data.extend(data_boundary_conditions_0)\n",
        "  total_input_train_data.extend(data_boundary_conditions_1)\n",
        "  #total_input_train_data.extend(data_pde_loss)\n",
        "  total_input_train_data.extend(data_vel_bottom)\n",
        "\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  total_input_train_data_scaled= scaler.fit_transform(total_input_train_data)\n",
        "  #input_data_function_approximation = scaler.fit_transform(input_data_function_approximation)\n",
        "  #input_data_boundary_conditions_0 = scaler.fit_transform(input_data_boundary_conditions_0)\n",
        "  #input_data_boundary_conditions_1 = scaler.fit_transform(input_data_boundary_conditions_1)\n",
        "  #input_data_vel_bottom = scaler.fit_transform(input_data_vel_bottom)\n",
        "  #input_data_pde_loss = scaler.fit_transform(input_data_pde_loss)\n",
        "\n",
        "  #print(total_input_train_data_scaled[:,0])\n",
        "  #print(total_input_train_data_scaled[:,1])\n",
        "\n",
        "\n",
        "  input_data_function_approximation = total_input_train_data_scaled[0:data_size,:]\n",
        "  input_data_boundary_conditions_0 = total_input_train_data_scaled[data_size:2*data_size,:]\n",
        "  input_data_boundary_conditions_1 = total_input_train_data_scaled[2*data_size:3*data_size,:]\n",
        "  #input_data_pde_loss = total_input_train_data_scaled[3*data_size:4*data_size,:]\n",
        "  input_data_vel_bottom = total_input_train_data_scaled[4*data_size:5*data_size,:]\n",
        "\n",
        "  return input_data_function_approximation, input_data_boundary_conditions_0,input_data_boundary_conditions_1, input_data_vel_bottom"
      ],
      "metadata": {
        "id": "nc1FFxuPXiKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters and data"
      ],
      "metadata": {
        "id": "cLs2H0gQX-tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Define parameters\n",
        "num_epochs = 5000\n",
        "learning_rate = 10**-4\n",
        "momentum = 0.9\n",
        "#alpha = 1.0  # Weight for data loss\n",
        "#beta = 1.0   # Weight for periodic boundary condition loss\n",
        "#epsilon = 1.0 # Weight for bottom velocity boundary condition loss\n",
        "#gamma = 1.0  # Weight for PDE loss\n",
        "\n",
        "\n",
        "# Create the model and optimizer\n",
        "model = FNN()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "training_data_size=20\n",
        "test_data_size=5\n",
        "\n",
        "# Generate training data for each component of the PINN loss\n",
        "input_data_function_approximation, target_data_function_approximation = generate_data_function_approximation(training_data_size)\n",
        "input_data_boundary_conditions_0, input_data_boundary_conditions_1 = generate_periodic_boundary_data(training_data_size, x1_values=[0, 2])\n",
        "#input_data_pde_loss = generate_data_pde_loss(training_data_size)\n",
        "input_data_vel_bottom = generate_data_bottom_velocity(training_data_size)\n",
        "\n",
        "\n",
        "\n",
        "#normalize data\n",
        "input_data_function_approximation, input_data_boundary_conditions_0,input_data_boundary_conditions_1, input_data_vel_bottom = normalize_data_no_PDE(input_data_function_approximation, input_data_boundary_conditions_0,input_data_boundary_conditions_1, input_data_vel_bottom,training_data_size)\n",
        "\n",
        "# Change 1: Create a SoftAdapt object (with your desired variant)\n",
        "softadapt_object = LossWeightedSoftAdapt(beta=0.1)\n",
        "\n",
        "# Change 2: Define how often SoftAdapt calculate weights for the loss components\n",
        "epochs_to_make_updates = 5\n",
        "\n",
        "# Change 3: Initialize lists to keep track of loss values over the epochs we defined above\n",
        "values_of_component_1 = []\n",
        "values_of_component_2 = []\n",
        "#values_of_component_3 = []\n",
        "values_of_component_4 = []\n",
        "# Initializing adaptive weights to all ones.\n",
        "adapt_weights = torch.tensor([1,1,1])"
      ],
      "metadata": {
        "id": "KzNd4ivzX-UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training loop"
      ],
      "metadata": {
        "id": "z53Yx_1YYb25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store training losses for plotting\n",
        "#loss_pde_list = []\n",
        "#loss_bc_list = []\n",
        "loss_data_list = []\n",
        "total_loss_list = []\n",
        "loss_per_bc_list=[]\n",
        "loss_bot_vel_bc_list=[]\n",
        "\n",
        "# Lists to store test losses for plotting\n",
        "#test_loss_pde_list = []\n",
        "#test_loss_bc_list = []\n",
        "test_loss_data_list_no_PDE = []\n",
        "test_total_loss_list_no_PDE = []\n",
        "test_loss_per_bc_list=[]\n",
        "test_loss_bot_vel_bc_list=[]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "    # Forward pass for each component\n",
        "    predicted_data_function_approximation = model(torch.from_numpy(input_data_function_approximation).float())\n",
        "\n",
        "    # Compute losses\n",
        "    loss_function_approximation = data_loss(predicted_data_function_approximation, torch.Tensor(target_data_function_approximation).float())\n",
        "\n",
        "    boundary_conditions = {\n",
        "      'bc1': compute_bc1(model, input_data_vel_bottom[:,0], input_data_vel_bottom[:,1]),   # at x2=-1 the grad perpendicular to the bottom is null\n",
        "      'bc2_0': compute_bc2(model,torch.Tensor( input_data_boundary_conditions_0)),  # at x1 = 0\n",
        "      'bc2_1': compute_bc2(model,torch.Tensor( input_data_boundary_conditions_1)),  # at x1 = 2\n",
        "    }\n",
        "\n",
        "    #loss_boundary_conditions = beta* boundary_condition_loss(boundary_conditions,predicted_data_function_approximation)\n",
        "\n",
        "    loss_periodic_boundary = periodic_boundary_loss(boundary_conditions)\n",
        "\n",
        "    #loss_pde_loss= pde_loss(model,torch.tensor(input_data_pde_loss, requires_grad = True, dtype=torch.float))\n",
        "\n",
        "    loss_vel_bottom_boundary = bottom_velocity_boundary_loss(boundary_conditions,predicted_data_function_approximation)\n",
        "\n",
        "    # Keeping track of each loss component\n",
        "    values_of_component_1.append(loss_function_approximation)\n",
        "    values_of_component_2.append( loss_periodic_boundary )\n",
        "    #values_of_component_3.append(loss_pde_loss)\n",
        "    values_of_component_4.append(loss_vel_bottom_boundary)\n",
        "\n",
        "    # Change 4: Make sure `epochs_to_make_change` have passed before calling SoftAdapt.\n",
        "    if epoch % epochs_to_make_updates == 0 and epoch != 0:\n",
        "        adapt_weights = softadapt_object.get_component_weights(torch.tensor(values_of_component_1),\n",
        "                                                                 torch.tensor(values_of_component_2),\n",
        "                                                                torch.tensor(values_of_component_4),\n",
        "                                                                 verbose=False,\n",
        "                                                                   )\n",
        "        # Resetting the lists to start fresh (this part is optional)\n",
        "        values_of_component_1 = []\n",
        "        values_of_component_2 = []\n",
        "        #values_of_component_3 = []\n",
        "        values_of_component_4 = []\n",
        "\n",
        "\n",
        "\n",
        "    # Total loss\n",
        "    #total_loss = loss_function_approximation + loss_boundary_conditions + loss_pde_loss\n",
        "\n",
        "    #total_loss = loss_function_approximation + loss_periodic_boundary + loss_vel_bottom_boundary + loss_pde_loss\n",
        "\n",
        "    total_loss = adapt_weights[0] * loss_function_approximation + adapt_weights[1]*loss_periodic_boundary + adapt_weights[2]*loss_vel_bottom_boundary\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    # Append losses to the lists\n",
        "    loss_pde_list.append(loss_pde_loss.item())\n",
        "    #loss_bc_list.append(loss_boundary_conditions.item())\n",
        "    loss_per_bc_list.append(loss_periodic_boundary.item())\n",
        "    loss_bot_vel_bc_list.append(loss_vel_bottom_boundary.item())\n",
        "    loss_data_list.append(loss_function_approximation.item())\n",
        "    total_loss_list_no_PDE.append(total_loss.item())\n",
        "\n",
        "    # Generate test data for each component of the PINN loss\n",
        "    test_input_data_function_approximation, test_target_data_function_approximation = generate_data_function_approximation(test_data_size)\n",
        "    test_input_data_boundary_conditions_0, test_input_data_boundary_conditions_1 = generate_periodic_boundary_data(test_data_size, x1_values=[0, 2])\n",
        "    test_input_data_pde_loss = generate_data_pde_loss(test_data_size)\n",
        "    test_input_data_vel_bottom = generate_data_bottom_velocity(test_data_size)\n",
        "\n",
        "    #normalize data\n",
        "    test_input_data_function_approximation, test_input_data_boundary_conditions_0, test_input_data_boundary_conditions_1, test_input_data_pde_loss, test_input_data_vel_bottom = normalize_data(test_input_data_function_approximation, test_input_data_boundary_conditions_0, test_input_data_boundary_conditions_1, test_input_data_pde_loss, test_input_data_vel_bottom,test_data_size)\n",
        "    # Test loop\n",
        "    # Forward pass for each component\n",
        "    test_predicted_data_function_approximation = model(torch.Tensor(test_input_data_function_approximation))\n",
        "\n",
        "    # Compute test losses\n",
        "    test_loss_function_approximation = data_loss(test_predicted_data_function_approximation, torch.Tensor(test_target_data_function_approximation))\n",
        "\n",
        "    test_boundary_conditions = {\n",
        "        'bc1': compute_bc1(model, test_input_data_vel_bottom[:, 0], test_input_data_vel_bottom[:, 1]),\n",
        "        'bc2_0': compute_bc2(model, torch.Tensor(test_input_data_boundary_conditions_0)),\n",
        "        'bc2_1': compute_bc2(model, torch.Tensor(test_input_data_boundary_conditions_1)),\n",
        "    }\n",
        "\n",
        "    #test_loss_boundary_conditions = beta*boundary_condition_loss(test_boundary_conditions, test_predicted_data_function_approximation)\n",
        "\n",
        "    test_loss_periodic_boundary = periodic_boundary_loss(test_boundary_conditions)\n",
        "\n",
        "    test_loss_vel_bottom_boundary = bottom_velocity_boundary_loss(test_boundary_conditions,test_predicted_data_function_approximation)\n",
        "\n",
        "\n",
        "    #test_loss_pde_loss = pde_loss(model, torch.tensor(test_input_data_pde_loss, requires_grad = True, dtype=torch.float))\n",
        "\n",
        "    # Total test loss\n",
        "    #test_total_loss = test_loss_function_approximation + test_loss_boundary_conditions + test_loss_pde_loss\n",
        "    test_total_loss_no_PDE = test_loss_function_approximation + test_loss_periodic_boundary + test_loss_vel_bottom_boundary\n",
        "\n",
        "    # Append test losses to the lists\n",
        "    #test_loss_pde_list.append(test_loss_pde_loss.item())\n",
        "    #test_loss_bc_list.append(test_loss_boundary_conditions.item())\n",
        "    test_loss_bot_vel_bc_list.append(test_loss_vel_bottom_boundary.item())\n",
        "    test_loss_per_bc_list.append(test_loss_periodic_boundary.item())\n",
        "    test_loss_data_list.append(test_loss_function_approximation.item())\n",
        "    test_total_loss_list_no_PDE.append(test_total_loss.item())\n",
        "\n",
        "    # Print loss for monitoring\n",
        "    if epoch % 100 == 0:\n",
        "        #print(f'Epoch {epoch}/{num_epochs},train PDE Loss: {loss_pde_loss}')\n",
        "        #print(f'Epoch {epoch}/{num_epochs},train Boundaries Loss: {loss_boundary_conditions}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},train Periodic Boundary Loss: {loss_periodic_boundary}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},train Bottom Velocity Boundary Loss: {loss_vel_bottom_boundary}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},train Data Loss: {loss_function_approximation}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},train Total Loss: {total_loss.item()}')\n",
        "        print(\"------------------------------------------------------------\")\n",
        "        #print(f'Epoch {epoch}/{num_epochs},test PDE Loss: {test_loss_pde_loss}')\n",
        "        #print(f'Epoch {epoch}/{num_epochs},test Boundaries Loss: {test_loss_boundary_conditions}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},test Periodic Boundary Loss: {test_loss_periodic_boundary}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},test Bottom Velocity Boundary Loss: {test_loss_vel_bottom_boundary}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},test Data Loss: {test_loss_function_approximation}')\n",
        "        print(f'Epoch {epoch}/{num_epochs},test Total Loss: {test_total_loss.item()}')\n",
        "        print(\"#############################################################\")\n",
        "        print(\"#############################################################\")\n",
        "# After training, you can use the trained model for predictions or further analysis\n"
      ],
      "metadata": {
        "id": "QUSgyXcSYU-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYNwSyoEZeME"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}