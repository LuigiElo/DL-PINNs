{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIXk2VADz8696UTvch0bzg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuigiElo/DL-PINNs/blob/main/PINN_LaPlace2D_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Network"
      ],
      "metadata": {
        "id": "dFr0R0YdjZy7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mEp-TPTfBeuF",
        "outputId": "96c39348-60b8-480b-a4a3-c63d43c026fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel = LinearNN()\\ninput_tensor = torch.randn((10, 2))  # Example input with batch size 10 and 2 features\\noutput = model(input_tensor)\\n\\nprint(\"Input shape:\", input_tensor)\\nprint(\"Output shape:\", output)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SineActivation(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)\n",
        "\n",
        "class LinearNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_inputs: int = 2,\n",
        "        num_layers: int = 3,\n",
        "        num_neurons: int = 32,\n",
        "        act: nn.Module = SineActivation(),\n",
        "    ) -> None:\n",
        "        \"\"\"Basic neural network architecture with linear layers\n",
        "\n",
        "        Args:\n",
        "            num_inputs (int, optional): the dimensionality of the input tensor\n",
        "            num_layers (int, optional): the number of hidden layers\n",
        "            num_neurons (int, optional): the number of neurons for each hidden layer\n",
        "            act (nn.Module, optional): the non-linear activation function to use for stitching\n",
        "                linear layers togeter\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_neurons = num_neurons\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # input layer\n",
        "        layers.append(nn.Linear(self.num_inputs, num_neurons))\n",
        "\n",
        "        # hidden layers with linear layer and activation\n",
        "        for _ in range(num_layers):\n",
        "            layers.extend([nn.Linear(num_neurons, num_neurons), act])\n",
        "\n",
        "        # output layer\n",
        "        layers.append(nn.Linear(num_neurons, 1))\n",
        "\n",
        "        # build the network\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x).squeeze()\n",
        "\n",
        "#network\n",
        "\n",
        "# Example usage:\n",
        "\"\"\"\n",
        "model = LinearNN()\n",
        "input_tensor = torch.randn((10, 2))  # Example input with batch size 10 and 2 features\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(\"Input shape:\", input_tensor)\n",
        "print(\"Output shape:\", output)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PINN Loss"
      ],
      "metadata": {
        "id": "0EgdELmFjcdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def data_loss(predicted_data, target_data): #data has to be in z=0, 0<x<L\n",
        "        #target data is obtained using behaviour function\n",
        "        return F.mse_loss(predicted_data, target_data)\n",
        "\n",
        "def boundary_condition_loss(boundary_conditions):\n",
        "    # Customize based on your specific boundary conditions\n",
        "    # Example: Dirichlet boundary condition u(0, t) = g(t)\n",
        "    loss_bc = F.mse_loss(boundary_conditions['bc1'], 0) + F.mse_loss(boundary_conditions['bc2_0'], boundary_conditions['bc2_0'])\n",
        "    # instead of 0 it will be an array of zeros with the same length as the number of points we decide to use to calculate the loss\n",
        "    return loss_bc\n",
        "\n",
        "def pde_loss(model,input_data): #here input_data has to be in -h(x)<x2<0\n",
        "\n",
        "    x1, x2 = input_data[:, 0], input_data[:, 1]\n",
        "\n",
        "    # Forward pass to get the function values\n",
        "    u = model(input_data)\n",
        "\n",
        "    # Compute the Hessian matrix\n",
        "    hessian = compute_hessian(u, input_data)\n",
        "\n",
        "    # Extract elements corresponding to (0, 0) and (1, 1)\n",
        "    hessian_00 = hessian[:, 0, 0]\n",
        "    hessian_11 = hessian[:, 1, 1]\n",
        "\n",
        "    # Enforce the PDE constraint\n",
        "    pde_residual = hessian_00 + hessian_11\n",
        "\n",
        "\n",
        "    return F.mse_loss(pde_residual, torch.zeros_like(pde_residual.size))\n",
        "\n",
        "def compute_bc1(model, x1, x2):\n",
        "    # Concatenate x1 and x2 to form the input tensor\n",
        "    input_tensor = torch.stack([x1, x2], dim=1)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    output = model(input_tensor)\n",
        "\n",
        "    # Compute the gradient with respect to x2\n",
        "    grad_x2 = torch.autograd.grad(output, x2, create_graph=True)[0]\n",
        "\n",
        "    return grad_x2\n",
        "\n",
        "def compute_bc2(model, x1, x2):\n",
        "    # Concatenate x1 and x2 to form the input tensor\n",
        "    input_tensor = torch.stack([x1, x2], dim=1)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    output = model(input_tensor)\n",
        "\n",
        "    return output\n",
        "\n",
        "def compute_hessian(output, input_data):\n",
        "    # Compute the Hessian matrix\n",
        "    hessian = autograd.functional.hessian(lambda x: output.sum(), input_data)\n",
        "\n",
        "    return hessian\n",
        "\n",
        "#x1 and x2 have to represent the points we want to apply the constraints\n",
        "\n",
        "boundary_conditions = {\n",
        "    'bc1': compute_bc1(model, x1, x2),   # at x2=-1 the grad perpendicular to the bottom is null\n",
        "    'bc2_0': compute_bc2(model, 0, x2),  # at x1 = 0\n",
        "    'bc2_2': compute_bc2(model, 2, x2),  # at x1 = 2\n",
        "}\n",
        "\n",
        "#total_loss = loss_data + loss_bc + loss_pde\n",
        "\n",
        "#the data we provide to calculate each loss is extremely important. It has to belong to the domain where the constraints/pde/function are applied."
      ],
      "metadata": {
        "id": "MqeERJMqgnsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# General parameters\n",
        "L = 2.0 # wave/x length\n",
        "h = 1 # depth\n",
        "T = 1 # period\n",
        "H = 0.05 # surface wave amplitude (I chose it arbitrarliy. Check p. 68 HRC)\n",
        "c = L/T # wave propagation velocity (p. 70 HCR)\n",
        "k = 2*np.pi/L # wave nr.\n",
        "w = 2*np.pi/T\n",
        "\n",
        "t = 0 # we are looking at a snapshot, so t is constant\n",
        "\n",
        "# Analytical solution (HRC p. 75)\n",
        "def behaviour_func(x):\n",
        "    return -H*c/2 * np.cosh(k*(x[:, 1:2]+h))/np.sinh(k*h) * np.sin(w*t - k*x[:, 0:1])\n"
      ],
      "metadata": {
        "id": "5hQNB4RuB0Wg"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}